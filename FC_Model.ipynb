{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import data_handle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入并处理数据\n",
    "1.读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, dummies, weight, label = data_handle.read_data('data/stock_train_data_20170910.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.将数据平均化，并去除极端值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, scaled_features = data_handle.scale_feature(X,dummies,quantile_percent=0.995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.将数据进行随机分组，分成测试集与训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (289506, 116) \n",
      " Y_train shape: (289506,) \n",
      " X_test shape: (32168, 116) \n",
      " Y_test shape: (32168,)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data_handle.data_split(X, label, test_size=0.1)\n",
    "print('X_train shape:',X_train.shape,'\\n',\n",
    "     'Y_train shape:', Y_train.shape,'\\n',\n",
    "     'X_test shape:', X_test.shape,'\\n',\n",
    "     'Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.定义分批获取数据函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(X, Y, batch_size):\n",
    "    data_len = len(X)\n",
    "    for i in range(0, data_len, batch_size):\n",
    "        end = i + batch_size\n",
    "        if end > data_len:\n",
    "            end = -1\n",
    "        x = X[i: end].reshape(-1,X.shape[1])\n",
    "        #print(x.shape)\n",
    "        y = Y[i : end].reshape(-1,1)\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(num_features):\n",
    "    '''\n",
    "    构建输入\n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.float32, [None, num_features], name='inputs')\n",
    "    targets = tf.placeholder(tf.float32, [None, 1], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_model(inputs,keep_prob):\n",
    "    layer1 = tf.layers.dense(inputs,58,activation=tf.nn.relu,kernel_initializer=tf.truncated_normal_initializer())\n",
    "    dropout = tf.nn.dropout(layer1,keep_prob)\n",
    "    layer2 = tf.layers.dense(dropout,29,activation=tf.nn.relu,kernel_initializer=tf.truncated_normal_initializer())\n",
    "    dropout = tf.nn.dropout(layer2,keep_prob)\n",
    "    layer3 = tf.layers.dense(dropout,14,activation=tf.nn.relu,kernel_initializer=tf.truncated_normal_initializer())\n",
    "    dropout = tf.nn.dropout(layer3,keep_prob)\n",
    "    logits = tf.layers.dense(dropout,1,activation=None,kernel_initializer=tf.truncated_normal_initializer(), name='logits')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_train,Y_train,X_test,Y_test,keep_prob,epoch_count, batch_size, learning_rate=0.001, num_features=116):\n",
    "    inputs, targets, k_p = build_inputs(num_features)\n",
    "    logits = fc_model(inputs,k_p)\n",
    "    out = tf.sigmoid(logits)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=targets))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(out), tf.int32), tf.cast(targets, tf.int32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    steps = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch_i in range(epoch_count):\n",
    "            for x,y in get_batches(X_train,Y_train,batch_size):\n",
    "                steps += 1\n",
    "                _, train_loss, train_accuracy = sess.run([train_opt, loss, accuracy], feed_dict={inputs:x, targets:y, k_p:keep_prob})\n",
    "                \n",
    "                if steps % 1000 == 0:\n",
    "                    test_loss, test_accuracy = sess.run([loss, accuracy], feed_dict={inputs:X_test.reshape(-1,num_features),\n",
    "                                                                                     targets:Y_test.reshape(-1,1), k_p:1.0})\n",
    "                    print(\"Epoch {}/{}.\".format(epoch_i+1, epoch_count),\n",
    "                          \"train_loss: {:.4f}..\".format(train_loss),\n",
    "                          \"train_acc: {:.4f}..\".format(train_accuracy),\n",
    "                          \"test_loss:{:.4f}..\".format(test_loss),\n",
    "                          \"test_acc:{:.4f}..\".format(test_accuracy))\n",
    "                    \n",
    "        data = pd.read_csv('data/stock_test_data_20170910.csv')\n",
    "        dummies = pd.get_dummies(data['group'], prefix='group', drop_first=False)\n",
    "        X = data.drop(['group','id'],axis=1)\n",
    "        for each in X.columns:\n",
    "            X.loc[:, each] = (X[each] - scaled_features[each][0])/scaled_features[each][1]\n",
    "            X.loc[X[each]>X[each].quantile(0.995)] = X[each].quantile(0.995)\n",
    "                              \n",
    "        X = pd.concat([X, dummies], axis=1).values\n",
    "        output = sess.run(out, feed_dict={inputs:X.reshape(-1,116),k_p:1.0})\n",
    "        print(len(output))\n",
    "        print(len(data))\n",
    "        data['proba'] = output\n",
    "        data[['id','proba']].to_csv('proba.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/800. train_loss: 13.4332.. train_acc: 0.5030.. test_loss:4.5527.. test_acc:0.5145..\n",
      "Epoch 7/800. train_loss: 2.0233.. train_acc: 0.4960.. test_loss:0.7463.. test_acc:0.4780..\n",
      "Epoch 11/800. train_loss: 0.8968.. train_acc: 0.4750.. test_loss:0.6932.. test_acc:0.5130..\n",
      "Epoch 14/800. train_loss: 1.0306.. train_acc: 0.5430.. test_loss:0.6916.. test_acc:0.5300..\n",
      "Epoch 18/800. train_loss: 0.7090.. train_acc: 0.5380.. test_loss:0.6911.. test_acc:0.5300..\n",
      "Epoch 21/800. train_loss: 0.7137.. train_acc: 0.5460.. test_loss:0.6913.. test_acc:0.5300..\n",
      "Epoch 25/800. train_loss: 0.7151.. train_acc: 0.5450.. test_loss:0.6910.. test_acc:0.5300..\n",
      "Epoch 28/800. train_loss: 0.6932.. train_acc: 0.5230.. test_loss:0.6912.. test_acc:0.5300..\n",
      "Epoch 32/800. train_loss: 0.7041.. train_acc: 0.5290.. test_loss:0.6910.. test_acc:0.5300..\n",
      "Epoch 35/800. train_loss: 0.7426.. train_acc: 0.5020.. test_loss:0.6910.. test_acc:0.5300..\n",
      "Epoch 38/800. train_loss: 0.6954.. train_acc: 0.4940.. test_loss:0.6910.. test_acc:0.5300..\n",
      "Epoch 42/800. train_loss: 0.6919.. train_acc: 0.5160.. test_loss:0.6901.. test_acc:0.5300..\n",
      "Epoch 45/800. train_loss: 0.6888.. train_acc: 0.5440.. test_loss:0.6898.. test_acc:0.5300..\n",
      "Epoch 49/800. train_loss: 0.6880.. train_acc: 0.5420.. test_loss:0.6892.. test_acc:0.5300..\n",
      "Epoch 52/800. train_loss: 0.6878.. train_acc: 0.5530.. test_loss:0.6885.. test_acc:0.5320..\n",
      "Epoch 56/800. train_loss: 0.6919.. train_acc: 0.5260.. test_loss:0.6874.. test_acc:0.5300..\n",
      "Epoch 59/800. train_loss: 0.6926.. train_acc: 0.5150.. test_loss:0.6876.. test_acc:0.5299..\n",
      "Epoch 63/800. train_loss: 0.6859.. train_acc: 0.5510.. test_loss:0.6857.. test_acc:0.5300..\n",
      "Epoch 66/800. train_loss: 0.6814.. train_acc: 0.5340.. test_loss:0.6838.. test_acc:0.5299..\n",
      "Epoch 69/800. train_loss: 0.6887.. train_acc: 0.5210.. test_loss:0.6836.. test_acc:0.5299..\n",
      "Epoch 73/800. train_loss: 0.6862.. train_acc: 0.5530.. test_loss:0.6824.. test_acc:0.5558..\n",
      "Epoch 76/800. train_loss: 0.6860.. train_acc: 0.5230.. test_loss:0.6801.. test_acc:0.5592..\n",
      "Epoch 80/800. train_loss: 0.6913.. train_acc: 0.5280.. test_loss:0.6805.. test_acc:0.5627..\n",
      "Epoch 83/800. train_loss: 0.6849.. train_acc: 0.5540.. test_loss:0.6801.. test_acc:0.5630..\n",
      "Epoch 87/800. train_loss: 0.6823.. train_acc: 0.5620.. test_loss:0.6796.. test_acc:0.5657..\n",
      "Epoch 90/800. train_loss: 0.6786.. train_acc: 0.5560.. test_loss:0.6797.. test_acc:0.5670..\n",
      "Epoch 94/800. train_loss: 0.6794.. train_acc: 0.5510.. test_loss:0.6774.. test_acc:0.5687..\n",
      "Epoch 97/800. train_loss: 0.6759.. train_acc: 0.5640.. test_loss:0.6777.. test_acc:0.5707..\n",
      "Epoch 100/800. train_loss: 0.6690.. train_acc: 0.5822.. test_loss:0.6752.. test_acc:0.5713..\n",
      "Epoch 104/800. train_loss: 0.6771.. train_acc: 0.5690.. test_loss:0.6759.. test_acc:0.5697..\n",
      "Epoch 107/800. train_loss: 0.6873.. train_acc: 0.5530.. test_loss:0.6747.. test_acc:0.5744..\n",
      "Epoch 111/800. train_loss: 0.6742.. train_acc: 0.5830.. test_loss:0.6749.. test_acc:0.5756..\n",
      "Epoch 114/800. train_loss: 0.6751.. train_acc: 0.5610.. test_loss:0.6723.. test_acc:0.5754..\n",
      "Epoch 118/800. train_loss: 0.6643.. train_acc: 0.6080.. test_loss:0.6721.. test_acc:0.5769..\n",
      "Epoch 121/800. train_loss: 0.6810.. train_acc: 0.5470.. test_loss:0.6706.. test_acc:0.5805..\n",
      "Epoch 125/800. train_loss: 0.6770.. train_acc: 0.5570.. test_loss:0.6694.. test_acc:0.5814..\n",
      "Epoch 128/800. train_loss: 0.6736.. train_acc: 0.5630.. test_loss:0.6694.. test_acc:0.5836..\n",
      "Epoch 132/800. train_loss: 0.6705.. train_acc: 0.5870.. test_loss:0.6685.. test_acc:0.5848..\n",
      "Epoch 135/800. train_loss: 0.6763.. train_acc: 0.5800.. test_loss:0.6668.. test_acc:0.5870..\n",
      "Epoch 138/800. train_loss: 0.6748.. train_acc: 0.5560.. test_loss:0.6692.. test_acc:0.5878..\n",
      "Epoch 142/800. train_loss: 0.6705.. train_acc: 0.5760.. test_loss:0.6665.. test_acc:0.5885..\n",
      "Epoch 145/800. train_loss: 0.6770.. train_acc: 0.5630.. test_loss:0.6683.. test_acc:0.5884..\n",
      "Epoch 149/800. train_loss: 0.6662.. train_acc: 0.6030.. test_loss:0.6638.. test_acc:0.5906..\n",
      "Epoch 152/800. train_loss: 0.6788.. train_acc: 0.5770.. test_loss:0.6666.. test_acc:0.5920..\n",
      "Epoch 156/800. train_loss: 0.6717.. train_acc: 0.5710.. test_loss:0.6642.. test_acc:0.5923..\n",
      "Epoch 159/800. train_loss: 0.6856.. train_acc: 0.5560.. test_loss:0.6647.. test_acc:0.5928..\n",
      "Epoch 163/800. train_loss: 0.6696.. train_acc: 0.5670.. test_loss:0.6644.. test_acc:0.5938..\n",
      "Epoch 166/800. train_loss: 0.6683.. train_acc: 0.5630.. test_loss:0.6610.. test_acc:0.5952..\n",
      "Epoch 169/800. train_loss: 0.6685.. train_acc: 0.5690.. test_loss:0.6621.. test_acc:0.5975..\n",
      "Epoch 173/800. train_loss: 0.6674.. train_acc: 0.5790.. test_loss:0.6603.. test_acc:0.5983..\n",
      "Epoch 176/800. train_loss: 0.6702.. train_acc: 0.5780.. test_loss:0.6597.. test_acc:0.5989..\n",
      "Epoch 180/800. train_loss: 0.6760.. train_acc: 0.5700.. test_loss:0.6593.. test_acc:0.5990..\n",
      "Epoch 183/800. train_loss: 0.6692.. train_acc: 0.5790.. test_loss:0.6607.. test_acc:0.6004..\n",
      "Epoch 187/800. train_loss: 0.6635.. train_acc: 0.6010.. test_loss:0.6611.. test_acc:0.6011..\n",
      "Epoch 190/800. train_loss: 0.6568.. train_acc: 0.6120.. test_loss:0.6593.. test_acc:0.6016..\n",
      "Epoch 194/800. train_loss: 0.6611.. train_acc: 0.5960.. test_loss:0.6584.. test_acc:0.6036..\n",
      "Epoch 197/800. train_loss: 0.6746.. train_acc: 0.5750.. test_loss:0.6580.. test_acc:0.6048..\n",
      "Epoch 200/800. train_loss: 0.6662.. train_acc: 0.5980.. test_loss:0.6566.. test_acc:0.6051..\n",
      "Epoch 204/800. train_loss: 0.6642.. train_acc: 0.6040.. test_loss:0.6567.. test_acc:0.6043..\n",
      "Epoch 207/800. train_loss: 0.6682.. train_acc: 0.5870.. test_loss:0.6548.. test_acc:0.6058..\n",
      "Epoch 211/800. train_loss: 0.6629.. train_acc: 0.5810.. test_loss:0.6557.. test_acc:0.6065..\n",
      "Epoch 214/800. train_loss: 0.6475.. train_acc: 0.6100.. test_loss:0.6543.. test_acc:0.6074..\n",
      "Epoch 218/800. train_loss: 0.6465.. train_acc: 0.6370.. test_loss:0.6547.. test_acc:0.6077..\n",
      "Epoch 221/800. train_loss: 0.6645.. train_acc: 0.5840.. test_loss:0.6532.. test_acc:0.6088..\n",
      "Epoch 225/800. train_loss: 0.6608.. train_acc: 0.5930.. test_loss:0.6523.. test_acc:0.6107..\n",
      "Epoch 228/800. train_loss: 0.6539.. train_acc: 0.5990.. test_loss:0.6522.. test_acc:0.6101..\n",
      "Epoch 232/800. train_loss: 0.6639.. train_acc: 0.5990.. test_loss:0.6510.. test_acc:0.6106..\n",
      "Epoch 235/800. train_loss: 0.6627.. train_acc: 0.5940.. test_loss:0.6507.. test_acc:0.6103..\n",
      "Epoch 238/800. train_loss: 0.6589.. train_acc: 0.5750.. test_loss:0.6510.. test_acc:0.6127..\n",
      "Epoch 242/800. train_loss: 0.6536.. train_acc: 0.6040.. test_loss:0.6517.. test_acc:0.6125..\n",
      "Epoch 245/800. train_loss: 0.6654.. train_acc: 0.5950.. test_loss:0.6509.. test_acc:0.6136..\n",
      "Epoch 249/800. train_loss: 0.6435.. train_acc: 0.6310.. test_loss:0.6482.. test_acc:0.6122..\n",
      "Epoch 252/800. train_loss: 0.6702.. train_acc: 0.5930.. test_loss:0.6496.. test_acc:0.6149..\n",
      "Epoch 256/800. train_loss: 0.6518.. train_acc: 0.6190.. test_loss:0.6470.. test_acc:0.6135..\n",
      "Epoch 259/800. train_loss: 0.6631.. train_acc: 0.5720.. test_loss:0.6475.. test_acc:0.6150..\n",
      "Epoch 263/800. train_loss: 0.6601.. train_acc: 0.5850.. test_loss:0.6475.. test_acc:0.6157..\n",
      "Epoch 266/800. train_loss: 0.6606.. train_acc: 0.5920.. test_loss:0.6475.. test_acc:0.6153..\n",
      "Epoch 269/800. train_loss: 0.6472.. train_acc: 0.5970.. test_loss:0.6474.. test_acc:0.6159..\n",
      "Epoch 273/800. train_loss: 0.6605.. train_acc: 0.5800.. test_loss:0.6469.. test_acc:0.6174..\n",
      "Epoch 276/800. train_loss: 0.6417.. train_acc: 0.6080.. test_loss:0.6444.. test_acc:0.6183..\n",
      "Epoch 280/800. train_loss: 0.6574.. train_acc: 0.5780.. test_loss:0.6436.. test_acc:0.6185..\n",
      "Epoch 283/800. train_loss: 0.6633.. train_acc: 0.6080.. test_loss:0.6451.. test_acc:0.6176..\n",
      "Epoch 287/800. train_loss: 0.6506.. train_acc: 0.6040.. test_loss:0.6450.. test_acc:0.6183..\n",
      "Epoch 290/800. train_loss: 0.6531.. train_acc: 0.5890.. test_loss:0.6444.. test_acc:0.6197..\n",
      "Epoch 294/800. train_loss: 0.6526.. train_acc: 0.5830.. test_loss:0.6450.. test_acc:0.6185..\n",
      "Epoch 297/800. train_loss: 0.6557.. train_acc: 0.6130.. test_loss:0.6439.. test_acc:0.6206..\n",
      "Epoch 300/800. train_loss: 0.6516.. train_acc: 0.6139.. test_loss:0.6426.. test_acc:0.6200..\n",
      "Epoch 304/800. train_loss: 0.6514.. train_acc: 0.5990.. test_loss:0.6429.. test_acc:0.6199..\n",
      "Epoch 307/800. train_loss: 0.6603.. train_acc: 0.5900.. test_loss:0.6403.. test_acc:0.6208..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 311/800. train_loss: 0.6528.. train_acc: 0.5970.. test_loss:0.6412.. test_acc:0.6216..\n",
      "Epoch 314/800. train_loss: 0.6540.. train_acc: 0.6040.. test_loss:0.6418.. test_acc:0.6220..\n",
      "Epoch 318/800. train_loss: 0.6373.. train_acc: 0.6410.. test_loss:0.6419.. test_acc:0.6217..\n",
      "Epoch 321/800. train_loss: 0.6560.. train_acc: 0.6000.. test_loss:0.6408.. test_acc:0.6239..\n",
      "Epoch 325/800. train_loss: 0.6387.. train_acc: 0.6060.. test_loss:0.6403.. test_acc:0.6234..\n",
      "Epoch 328/800. train_loss: 0.6451.. train_acc: 0.6100.. test_loss:0.6407.. test_acc:0.6239..\n",
      "Epoch 332/800. train_loss: 0.6662.. train_acc: 0.6070.. test_loss:0.6396.. test_acc:0.6244..\n",
      "Epoch 335/800. train_loss: 0.6518.. train_acc: 0.6080.. test_loss:0.6399.. test_acc:0.6242..\n",
      "Epoch 338/800. train_loss: 0.6415.. train_acc: 0.6250.. test_loss:0.6386.. test_acc:0.6248..\n",
      "Epoch 342/800. train_loss: 0.6454.. train_acc: 0.6170.. test_loss:0.6399.. test_acc:0.6243..\n",
      "Epoch 345/800. train_loss: 0.6550.. train_acc: 0.6060.. test_loss:0.6394.. test_acc:0.6250..\n",
      "Epoch 349/800. train_loss: 0.6426.. train_acc: 0.6180.. test_loss:0.6368.. test_acc:0.6252..\n",
      "Epoch 352/800. train_loss: 0.6540.. train_acc: 0.6280.. test_loss:0.6382.. test_acc:0.6269..\n",
      "Epoch 356/800. train_loss: 0.6388.. train_acc: 0.6240.. test_loss:0.6366.. test_acc:0.6255..\n",
      "Epoch 359/800. train_loss: 0.6599.. train_acc: 0.6030.. test_loss:0.6381.. test_acc:0.6262..\n",
      "Epoch 363/800. train_loss: 0.6470.. train_acc: 0.6070.. test_loss:0.6381.. test_acc:0.6259..\n",
      "Epoch 366/800. train_loss: 0.6496.. train_acc: 0.5970.. test_loss:0.6369.. test_acc:0.6277..\n",
      "Epoch 369/800. train_loss: 0.6479.. train_acc: 0.6020.. test_loss:0.6378.. test_acc:0.6275..\n",
      "Epoch 373/800. train_loss: 0.6478.. train_acc: 0.6020.. test_loss:0.6372.. test_acc:0.6281..\n",
      "Epoch 376/800. train_loss: 0.6313.. train_acc: 0.6350.. test_loss:0.6362.. test_acc:0.6283..\n",
      "Epoch 380/800. train_loss: 0.6541.. train_acc: 0.5910.. test_loss:0.6349.. test_acc:0.6297..\n",
      "Epoch 383/800. train_loss: 0.6440.. train_acc: 0.6050.. test_loss:0.6373.. test_acc:0.6282..\n",
      "Epoch 387/800. train_loss: 0.6423.. train_acc: 0.6330.. test_loss:0.6352.. test_acc:0.6285..\n",
      "Epoch 390/800. train_loss: 0.6500.. train_acc: 0.6060.. test_loss:0.6360.. test_acc:0.6288..\n",
      "Epoch 394/800. train_loss: 0.6309.. train_acc: 0.6360.. test_loss:0.6356.. test_acc:0.6304..\n",
      "Epoch 397/800. train_loss: 0.6561.. train_acc: 0.6180.. test_loss:0.6361.. test_acc:0.6297..\n",
      "Epoch 400/800. train_loss: 0.6141.. train_acc: 0.6515.. test_loss:0.6349.. test_acc:0.6306..\n",
      "Epoch 404/800. train_loss: 0.6487.. train_acc: 0.6230.. test_loss:0.6346.. test_acc:0.6303..\n",
      "Epoch 407/800. train_loss: 0.6641.. train_acc: 0.5830.. test_loss:0.6328.. test_acc:0.6294..\n",
      "Epoch 411/800. train_loss: 0.6507.. train_acc: 0.5920.. test_loss:0.6333.. test_acc:0.6294..\n",
      "Epoch 414/800. train_loss: 0.6418.. train_acc: 0.6340.. test_loss:0.6343.. test_acc:0.6305..\n",
      "Epoch 418/800. train_loss: 0.6232.. train_acc: 0.6450.. test_loss:0.6340.. test_acc:0.6305..\n",
      "Epoch 421/800. train_loss: 0.6528.. train_acc: 0.6020.. test_loss:0.6328.. test_acc:0.6313..\n",
      "Epoch 425/800. train_loss: 0.6322.. train_acc: 0.6260.. test_loss:0.6327.. test_acc:0.6313..\n",
      "Epoch 428/800. train_loss: 0.6285.. train_acc: 0.6460.. test_loss:0.6331.. test_acc:0.6314..\n",
      "Epoch 432/800. train_loss: 0.6604.. train_acc: 0.5980.. test_loss:0.6328.. test_acc:0.6313..\n",
      "Epoch 435/800. train_loss: 0.6535.. train_acc: 0.6070.. test_loss:0.6327.. test_acc:0.6309..\n",
      "Epoch 438/800. train_loss: 0.6276.. train_acc: 0.6490.. test_loss:0.6316.. test_acc:0.6312..\n",
      "Epoch 442/800. train_loss: 0.6336.. train_acc: 0.6350.. test_loss:0.6331.. test_acc:0.6311..\n",
      "Epoch 445/800. train_loss: 0.6361.. train_acc: 0.6310.. test_loss:0.6326.. test_acc:0.6308..\n",
      "Epoch 449/800. train_loss: 0.6205.. train_acc: 0.6440.. test_loss:0.6306.. test_acc:0.6320..\n",
      "Epoch 452/800. train_loss: 0.6602.. train_acc: 0.6000.. test_loss:0.6311.. test_acc:0.6321..\n",
      "Epoch 456/800. train_loss: 0.6209.. train_acc: 0.6380.. test_loss:0.6304.. test_acc:0.6318..\n",
      "Epoch 459/800. train_loss: 0.6395.. train_acc: 0.6240.. test_loss:0.6313.. test_acc:0.6312..\n",
      "Epoch 463/800. train_loss: 0.6376.. train_acc: 0.6180.. test_loss:0.6311.. test_acc:0.6329..\n",
      "Epoch 466/800. train_loss: 0.6397.. train_acc: 0.6130.. test_loss:0.6306.. test_acc:0.6319..\n",
      "Epoch 469/800. train_loss: 0.6476.. train_acc: 0.5930.. test_loss:0.6304.. test_acc:0.6323..\n",
      "Epoch 473/800. train_loss: 0.6426.. train_acc: 0.6140.. test_loss:0.6311.. test_acc:0.6321..\n",
      "Epoch 476/800. train_loss: 0.6185.. train_acc: 0.6500.. test_loss:0.6306.. test_acc:0.6317..\n",
      "Epoch 480/800. train_loss: 0.6265.. train_acc: 0.6350.. test_loss:0.6287.. test_acc:0.6333..\n",
      "Epoch 483/800. train_loss: 0.6340.. train_acc: 0.6280.. test_loss:0.6306.. test_acc:0.6329..\n",
      "Epoch 487/800. train_loss: 0.6395.. train_acc: 0.6210.. test_loss:0.6293.. test_acc:0.6327..\n",
      "Epoch 490/800. train_loss: 0.6458.. train_acc: 0.6260.. test_loss:0.6308.. test_acc:0.6337..\n",
      "Epoch 494/800. train_loss: 0.6428.. train_acc: 0.6220.. test_loss:0.6305.. test_acc:0.6338..\n",
      "Epoch 497/800. train_loss: 0.6406.. train_acc: 0.6180.. test_loss:0.6299.. test_acc:0.6336..\n",
      "Epoch 500/800. train_loss: 0.6250.. train_acc: 0.6436.. test_loss:0.6295.. test_acc:0.6335..\n",
      "Epoch 504/800. train_loss: 0.6421.. train_acc: 0.6290.. test_loss:0.6291.. test_acc:0.6338..\n",
      "Epoch 507/800. train_loss: 0.6450.. train_acc: 0.6240.. test_loss:0.6277.. test_acc:0.6339..\n",
      "Epoch 511/800. train_loss: 0.6386.. train_acc: 0.6130.. test_loss:0.6281.. test_acc:0.6342..\n",
      "Epoch 514/800. train_loss: 0.6398.. train_acc: 0.6230.. test_loss:0.6292.. test_acc:0.6349..\n",
      "Epoch 518/800. train_loss: 0.6256.. train_acc: 0.6530.. test_loss:0.6285.. test_acc:0.6334..\n",
      "Epoch 521/800. train_loss: 0.6444.. train_acc: 0.6280.. test_loss:0.6279.. test_acc:0.6339..\n",
      "Epoch 525/800. train_loss: 0.6243.. train_acc: 0.6280.. test_loss:0.6284.. test_acc:0.6345..\n",
      "Epoch 528/800. train_loss: 0.6276.. train_acc: 0.6310.. test_loss:0.6281.. test_acc:0.6334..\n",
      "Epoch 532/800. train_loss: 0.6431.. train_acc: 0.6030.. test_loss:0.6280.. test_acc:0.6353..\n",
      "Epoch 535/800. train_loss: 0.6520.. train_acc: 0.6130.. test_loss:0.6268.. test_acc:0.6355..\n",
      "Epoch 538/800. train_loss: 0.6297.. train_acc: 0.6270.. test_loss:0.6265.. test_acc:0.6352..\n",
      "Epoch 542/800. train_loss: 0.6220.. train_acc: 0.6430.. test_loss:0.6276.. test_acc:0.6356..\n",
      "Epoch 545/800. train_loss: 0.6356.. train_acc: 0.6190.. test_loss:0.6274.. test_acc:0.6363..\n",
      "Epoch 549/800. train_loss: 0.6023.. train_acc: 0.6660.. test_loss:0.6253.. test_acc:0.6358..\n",
      "Epoch 552/800. train_loss: 0.6407.. train_acc: 0.6260.. test_loss:0.6266.. test_acc:0.6370..\n",
      "Epoch 556/800. train_loss: 0.6256.. train_acc: 0.6360.. test_loss:0.6258.. test_acc:0.6361..\n",
      "Epoch 559/800. train_loss: 0.6426.. train_acc: 0.6180.. test_loss:0.6270.. test_acc:0.6369..\n",
      "Epoch 563/800. train_loss: 0.6247.. train_acc: 0.6380.. test_loss:0.6264.. test_acc:0.6369..\n",
      "Epoch 566/800. train_loss: 0.6511.. train_acc: 0.5920.. test_loss:0.6254.. test_acc:0.6375..\n",
      "Epoch 569/800. train_loss: 0.6405.. train_acc: 0.6170.. test_loss:0.6259.. test_acc:0.6377..\n",
      "Epoch 573/800. train_loss: 0.6352.. train_acc: 0.6280.. test_loss:0.6257.. test_acc:0.6373..\n",
      "Epoch 576/800. train_loss: 0.6218.. train_acc: 0.6390.. test_loss:0.6251.. test_acc:0.6375..\n",
      "Epoch 580/800. train_loss: 0.6297.. train_acc: 0.6250.. test_loss:0.6242.. test_acc:0.6365..\n",
      "Epoch 583/800. train_loss: 0.6222.. train_acc: 0.6410.. test_loss:0.6258.. test_acc:0.6380..\n",
      "Epoch 587/800. train_loss: 0.6317.. train_acc: 0.6400.. test_loss:0.6246.. test_acc:0.6389..\n",
      "Epoch 590/800. train_loss: 0.6431.. train_acc: 0.6080.. test_loss:0.6254.. test_acc:0.6383..\n",
      "Epoch 594/800. train_loss: 0.6329.. train_acc: 0.6370.. test_loss:0.6259.. test_acc:0.6382..\n",
      "Epoch 597/800. train_loss: 0.6398.. train_acc: 0.6220.. test_loss:0.6247.. test_acc:0.6385..\n",
      "Epoch 600/800. train_loss: 0.6136.. train_acc: 0.6416.. test_loss:0.6250.. test_acc:0.6394..\n",
      "Epoch 604/800. train_loss: 0.6314.. train_acc: 0.6490.. test_loss:0.6244.. test_acc:0.6398..\n",
      "Epoch 607/800. train_loss: 0.6422.. train_acc: 0.6280.. test_loss:0.6232.. test_acc:0.6397..\n",
      "Epoch 611/800. train_loss: 0.6352.. train_acc: 0.6040.. test_loss:0.6235.. test_acc:0.6394..\n",
      "Epoch 614/800. train_loss: 0.6308.. train_acc: 0.6250.. test_loss:0.6244.. test_acc:0.6397..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 618/800. train_loss: 0.6112.. train_acc: 0.6680.. test_loss:0.6240.. test_acc:0.6398..\n",
      "Epoch 621/800. train_loss: 0.6332.. train_acc: 0.6420.. test_loss:0.6239.. test_acc:0.6405..\n",
      "Epoch 625/800. train_loss: 0.6296.. train_acc: 0.6370.. test_loss:0.6248.. test_acc:0.6403..\n",
      "Epoch 628/800. train_loss: 0.6242.. train_acc: 0.6230.. test_loss:0.6240.. test_acc:0.6408..\n",
      "Epoch 632/800. train_loss: 0.6430.. train_acc: 0.6040.. test_loss:0.6241.. test_acc:0.6416..\n",
      "Epoch 635/800. train_loss: 0.6462.. train_acc: 0.6340.. test_loss:0.6233.. test_acc:0.6409..\n",
      "Epoch 638/800. train_loss: 0.6301.. train_acc: 0.6160.. test_loss:0.6224.. test_acc:0.6416..\n",
      "Epoch 642/800. train_loss: 0.6295.. train_acc: 0.6340.. test_loss:0.6234.. test_acc:0.6417..\n",
      "Epoch 645/800. train_loss: 0.6294.. train_acc: 0.6320.. test_loss:0.6235.. test_acc:0.6404..\n",
      "Epoch 649/800. train_loss: 0.6077.. train_acc: 0.6490.. test_loss:0.6222.. test_acc:0.6403..\n",
      "Epoch 652/800. train_loss: 0.6330.. train_acc: 0.6240.. test_loss:0.6224.. test_acc:0.6411..\n",
      "Epoch 656/800. train_loss: 0.6097.. train_acc: 0.6510.. test_loss:0.6222.. test_acc:0.6412..\n",
      "Epoch 659/800. train_loss: 0.6369.. train_acc: 0.6430.. test_loss:0.6226.. test_acc:0.6421..\n",
      "Epoch 663/800. train_loss: 0.6227.. train_acc: 0.6410.. test_loss:0.6227.. test_acc:0.6414..\n",
      "Epoch 666/800. train_loss: 0.6291.. train_acc: 0.6190.. test_loss:0.6221.. test_acc:0.6427..\n",
      "Epoch 669/800. train_loss: 0.6444.. train_acc: 0.6260.. test_loss:0.6227.. test_acc:0.6433..\n",
      "Epoch 673/800. train_loss: 0.6442.. train_acc: 0.6220.. test_loss:0.6222.. test_acc:0.6426..\n",
      "Epoch 676/800. train_loss: 0.6064.. train_acc: 0.6590.. test_loss:0.6218.. test_acc:0.6416..\n",
      "Epoch 680/800. train_loss: 0.6261.. train_acc: 0.6390.. test_loss:0.6205.. test_acc:0.6435..\n",
      "Epoch 683/800. train_loss: 0.6205.. train_acc: 0.6310.. test_loss:0.6225.. test_acc:0.6425..\n",
      "Epoch 687/800. train_loss: 0.6234.. train_acc: 0.6500.. test_loss:0.6212.. test_acc:0.6433..\n",
      "Epoch 690/800. train_loss: 0.6431.. train_acc: 0.6160.. test_loss:0.6225.. test_acc:0.6428..\n",
      "Epoch 694/800. train_loss: 0.6286.. train_acc: 0.6260.. test_loss:0.6222.. test_acc:0.6423..\n",
      "Epoch 697/800. train_loss: 0.6215.. train_acc: 0.6500.. test_loss:0.6209.. test_acc:0.6435..\n",
      "Epoch 700/800. train_loss: 0.6083.. train_acc: 0.6376.. test_loss:0.6218.. test_acc:0.6436..\n",
      "Epoch 704/800. train_loss: 0.6311.. train_acc: 0.6390.. test_loss:0.6213.. test_acc:0.6437..\n",
      "Epoch 707/800. train_loss: 0.6347.. train_acc: 0.6360.. test_loss:0.6202.. test_acc:0.6436..\n",
      "Epoch 711/800. train_loss: 0.6283.. train_acc: 0.6150.. test_loss:0.6206.. test_acc:0.6438..\n",
      "Epoch 714/800. train_loss: 0.6281.. train_acc: 0.6510.. test_loss:0.6212.. test_acc:0.6434..\n",
      "Epoch 718/800. train_loss: 0.6080.. train_acc: 0.6590.. test_loss:0.6206.. test_acc:0.6442..\n",
      "Epoch 721/800. train_loss: 0.6444.. train_acc: 0.6340.. test_loss:0.6211.. test_acc:0.6435..\n",
      "Epoch 725/800. train_loss: 0.6173.. train_acc: 0.6470.. test_loss:0.6210.. test_acc:0.6435..\n",
      "Epoch 728/800. train_loss: 0.6225.. train_acc: 0.6300.. test_loss:0.6205.. test_acc:0.6433..\n",
      "Epoch 732/800. train_loss: 0.6501.. train_acc: 0.6120.. test_loss:0.6208.. test_acc:0.6448..\n",
      "Epoch 735/800. train_loss: 0.6425.. train_acc: 0.6170.. test_loss:0.6205.. test_acc:0.6436..\n",
      "Epoch 738/800. train_loss: 0.6254.. train_acc: 0.6330.. test_loss:0.6195.. test_acc:0.6442..\n",
      "Epoch 742/800. train_loss: 0.6274.. train_acc: 0.6460.. test_loss:0.6201.. test_acc:0.6452..\n",
      "Epoch 745/800. train_loss: 0.6338.. train_acc: 0.6400.. test_loss:0.6204.. test_acc:0.6441..\n",
      "Epoch 749/800. train_loss: 0.6035.. train_acc: 0.6720.. test_loss:0.6193.. test_acc:0.6452..\n",
      "Epoch 752/800. train_loss: 0.6282.. train_acc: 0.6300.. test_loss:0.6200.. test_acc:0.6450..\n",
      "Epoch 756/800. train_loss: 0.6037.. train_acc: 0.6490.. test_loss:0.6192.. test_acc:0.6451..\n",
      "Epoch 759/800. train_loss: 0.6360.. train_acc: 0.6380.. test_loss:0.6195.. test_acc:0.6454..\n",
      "Epoch 763/800. train_loss: 0.6251.. train_acc: 0.6310.. test_loss:0.6199.. test_acc:0.6448..\n",
      "Epoch 766/800. train_loss: 0.6243.. train_acc: 0.6250.. test_loss:0.6190.. test_acc:0.6446..\n",
      "Epoch 769/800. train_loss: 0.6378.. train_acc: 0.6280.. test_loss:0.6192.. test_acc:0.6460..\n",
      "Epoch 773/800. train_loss: 0.6221.. train_acc: 0.6490.. test_loss:0.6192.. test_acc:0.6454..\n",
      "Epoch 776/800. train_loss: 0.6083.. train_acc: 0.6760.. test_loss:0.6186.. test_acc:0.6461..\n",
      "Epoch 780/800. train_loss: 0.6269.. train_acc: 0.6470.. test_loss:0.6180.. test_acc:0.6460..\n",
      "Epoch 783/800. train_loss: 0.6179.. train_acc: 0.6450.. test_loss:0.6202.. test_acc:0.6454..\n",
      "Epoch 787/800. train_loss: 0.6163.. train_acc: 0.6470.. test_loss:0.6188.. test_acc:0.6447..\n",
      "Epoch 790/800. train_loss: 0.6397.. train_acc: 0.6250.. test_loss:0.6196.. test_acc:0.6449..\n",
      "Epoch 794/800. train_loss: 0.6321.. train_acc: 0.6240.. test_loss:0.6200.. test_acc:0.6454..\n",
      "Epoch 797/800. train_loss: 0.6248.. train_acc: 0.6460.. test_loss:0.6188.. test_acc:0.6453..\n",
      "Epoch 800/800. train_loss: 0.6092.. train_acc: 0.6673.. test_loss:0.6186.. test_acc:0.6452..\n",
      "202757\n",
      "202757\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "learning_rate = 0.0003\n",
    "keep_prob = 0.80\n",
    "epochs = 800\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    train(X_train,Y_train,X_test,Y_test,keep_prob,epochs,batch_size,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
