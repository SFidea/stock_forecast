{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import data_handle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 载入并处理数据\n",
    "1.读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, dummies, weight, label = data_handle.read_data('data/stock_train_data_20170910.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.将数据平均化，并去除极端值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, scaled_features = data_handle.scale_feature(X,dummies,quantile_percent=0.995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.将数据进行随机分组，分成测试集与训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (289506, 116) \n",
      " Y_train shape: (289506,) \n",
      " X_test shape: (32168, 116) \n",
      " Y_test shape: (32168,)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data_handle.data_split(X, label, test_size=0.1)\n",
    "print('X_train shape:',X_train.shape,'\\n',\n",
    "     'Y_train shape:', Y_train.shape,'\\n',\n",
    "     'X_test shape:', X_test.shape,'\\n',\n",
    "     'Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.定义分批获取数据函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(X, Y, batch_size):\n",
    "    data_len = len(X)\n",
    "    for i in range(0, data_len, batch_size):\n",
    "        end = i + batch_size\n",
    "        if end > data_len:\n",
    "            end = -1\n",
    "        x = X[i: end].reshape(-1,X.shape[1])\n",
    "        #print(x.shape)\n",
    "        y = Y[i : end].reshape(-1,1)\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(num_features):\n",
    "    '''\n",
    "    构建输入\n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.float32, [None, num_features], name='inputs')\n",
    "    targets = tf.placeholder(tf.float32, [None, 1], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_model(inputs,keep_prob):\n",
    "    layer1 = tf.layers.dense(inputs,58,activation=tf.nn.relu,kernel_initializer=tf.truncated_normal_initializer())\n",
    "    dropout = tf.nn.dropout(layer1,keep_prob)\n",
    "    layer2 = tf.layers.dense(dropout,29,activation=tf.nn.relu,kernel_initializer=tf.truncated_normal_initializer())\n",
    "    dropout = tf.nn.dropout(layer2,keep_prob)\n",
    "    layer3 = tf.layers.dense(dropout,14,activation=tf.nn.relu,kernel_initializer=tf.truncated_normal_initializer())\n",
    "    dropout = tf.nn.dropout(layer3,keep_prob)\n",
    "    logits = tf.layers.dense(dropout,1,activation=None,kernel_initializer=tf.truncated_normal_initializer())\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_train,Y_train,X_test,Y_test,keep_prob,epoch_count, batch_size, learning_rate=0.001, num_features=116):\n",
    "    inputs, targets, k_p = build_inputs(num_features)\n",
    "    logits = fc_model(inputs,keep_prob)\n",
    "    out = tf.sigmoid(logits)\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=targets))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(out), tf.int32), tf.cast(targets, tf.int32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    steps = 0\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch_i in range(epoch_count):\n",
    "            for x,y in get_batches(X_train,Y_train,batch_size):\n",
    "                steps += 1\n",
    "                _, train_loss, train_accuracy = sess.run([train_opt, loss, accuracy], feed_dict={inputs:x, targets:y, k_p:keep_prob})\n",
    "                \n",
    "                if steps % 1000 == 0:\n",
    "                    test_loss, test_accuracy = sess.run([loss, accuracy], feed_dict={inputs:X_test.reshape(-1,num_features),\n",
    "                                                                                     targets:Y_test.reshape(-1,1), k_p:1.0})\n",
    "                    print(\"Epoch {}/{}.\".format(epoch_i+1, epoch_count),\n",
    "                          \"train_loss: {:.4f}..\".format(train_loss),\n",
    "                          \"train_acc: {:.4f}..\".format(train_accuracy),\n",
    "                          \"test_loss:{:.4f}..\".format(test_loss),\n",
    "                          \"test_acc:{:.4f}..\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200. train_loss: 14.5359.. train_acc: 0.5900.. test_loss:21.1278.. test_acc:0.5102..\n",
      "Epoch 1/200. train_loss: 8.4047.. train_acc: 0.5000.. test_loss:11.3755.. test_acc:0.5023..\n",
      "Epoch 2/200. train_loss: 6.0696.. train_acc: 0.4300.. test_loss:6.4924.. test_acc:0.4978..\n",
      "Epoch 2/200. train_loss: 2.6469.. train_acc: 0.5200.. test_loss:3.2698.. test_acc:0.4995..\n",
      "Epoch 2/200. train_loss: 1.5534.. train_acc: 0.5300.. test_loss:1.9545.. test_acc:0.4922..\n",
      "Epoch 3/200. train_loss: 2.0327.. train_acc: 0.5100.. test_loss:1.3635.. test_acc:0.5192..\n",
      "Epoch 3/200. train_loss: 0.7803.. train_acc: 0.5200.. test_loss:1.1073.. test_acc:0.5228..\n",
      "Epoch 3/200. train_loss: 1.1405.. train_acc: 0.5300.. test_loss:0.9394.. test_acc:0.5239..\n",
      "Epoch 4/200. train_loss: 0.7434.. train_acc: 0.5400.. test_loss:0.8421.. test_acc:0.5266..\n",
      "Epoch 4/200. train_loss: 0.6799.. train_acc: 0.5600.. test_loss:0.7967.. test_acc:0.5265..\n",
      "Epoch 4/200. train_loss: 0.6791.. train_acc: 0.5800.. test_loss:0.7405.. test_acc:0.5263..\n",
      "Epoch 5/200. train_loss: 0.7299.. train_acc: 0.5600.. test_loss:0.7238.. test_acc:0.5288..\n",
      "Epoch 5/200. train_loss: 0.6945.. train_acc: 0.5100.. test_loss:0.7221.. test_acc:0.5288..\n",
      "Epoch 5/200. train_loss: 0.6807.. train_acc: 0.6000.. test_loss:0.7098.. test_acc:0.5300..\n",
      "Epoch 6/200. train_loss: 0.7022.. train_acc: 0.4400.. test_loss:0.7021.. test_acc:0.5298..\n",
      "Epoch 6/200. train_loss: 0.6964.. train_acc: 0.4800.. test_loss:0.7027.. test_acc:0.5297..\n",
      "Epoch 6/200. train_loss: 0.7015.. train_acc: 0.4500.. test_loss:0.6993.. test_acc:0.5293..\n",
      "Epoch 7/200. train_loss: 0.6943.. train_acc: 0.5100.. test_loss:0.6971.. test_acc:0.5293..\n",
      "Epoch 7/200. train_loss: 0.7003.. train_acc: 0.5600.. test_loss:0.6949.. test_acc:0.5300..\n",
      "Epoch 7/200. train_loss: 0.6931.. train_acc: 0.5100.. test_loss:0.6993.. test_acc:0.5298..\n",
      "Epoch 8/200. train_loss: 0.6860.. train_acc: 0.5300.. test_loss:0.6973.. test_acc:0.5294..\n",
      "Epoch 8/200. train_loss: 0.6856.. train_acc: 0.5800.. test_loss:0.6926.. test_acc:0.5298..\n",
      "Epoch 8/200. train_loss: 0.6926.. train_acc: 0.5200.. test_loss:0.6919.. test_acc:0.5294..\n",
      "Epoch 9/200. train_loss: 0.6977.. train_acc: 0.5800.. test_loss:0.6930.. test_acc:0.5303..\n",
      "Epoch 9/200. train_loss: 0.6930.. train_acc: 0.5200.. test_loss:0.6937.. test_acc:0.5291..\n",
      "Epoch 9/200. train_loss: 0.6885.. train_acc: 0.5200.. test_loss:0.6933.. test_acc:0.5298..\n",
      "Epoch 10/200. train_loss: 0.6815.. train_acc: 0.5600.. test_loss:0.6923.. test_acc:0.5285..\n",
      "Epoch 10/200. train_loss: 0.6852.. train_acc: 0.5700.. test_loss:0.6916.. test_acc:0.5289..\n",
      "Epoch 11/200. train_loss: 0.6924.. train_acc: 0.5200.. test_loss:0.6918.. test_acc:0.5299..\n",
      "Epoch 11/200. train_loss: 0.6932.. train_acc: 0.5100.. test_loss:0.6937.. test_acc:0.5297..\n",
      "Epoch 11/200. train_loss: 0.7034.. train_acc: 0.4400.. test_loss:0.6933.. test_acc:0.5300..\n",
      "Epoch 12/200. train_loss: 0.6873.. train_acc: 0.5600.. test_loss:0.6920.. test_acc:0.5303..\n",
      "Epoch 12/200. train_loss: 0.6949.. train_acc: 0.5000.. test_loss:0.6928.. test_acc:0.5303..\n",
      "Epoch 12/200. train_loss: 0.6838.. train_acc: 0.6000.. test_loss:0.6930.. test_acc:0.5298..\n",
      "Epoch 13/200. train_loss: 0.6971.. train_acc: 0.4800.. test_loss:0.6927.. test_acc:0.5300..\n",
      "Epoch 13/200. train_loss: 0.6841.. train_acc: 0.5900.. test_loss:0.6919.. test_acc:0.5298..\n",
      "Epoch 13/200. train_loss: 0.6955.. train_acc: 0.4900.. test_loss:0.6919.. test_acc:0.5297..\n",
      "Epoch 14/200. train_loss: 0.6930.. train_acc: 0.5000.. test_loss:0.6922.. test_acc:0.5300..\n",
      "Epoch 14/200. train_loss: 0.6789.. train_acc: 0.5900.. test_loss:0.6919.. test_acc:0.5290..\n",
      "Epoch 14/200. train_loss: 0.6915.. train_acc: 0.5300.. test_loss:0.6914.. test_acc:0.5298..\n",
      "Epoch 15/200. train_loss: 0.6826.. train_acc: 0.6000.. test_loss:0.6913.. test_acc:0.5297..\n",
      "Epoch 15/200. train_loss: 0.6840.. train_acc: 0.5800.. test_loss:0.6918.. test_acc:0.5300..\n",
      "Epoch 15/200. train_loss: 0.6914.. train_acc: 0.5600.. test_loss:0.6913.. test_acc:0.5293..\n",
      "Epoch 16/200. train_loss: 0.6984.. train_acc: 0.4600.. test_loss:0.6915.. test_acc:0.5300..\n",
      "Epoch 16/200. train_loss: 0.6917.. train_acc: 0.5200.. test_loss:0.6914.. test_acc:0.5300..\n",
      "Epoch 16/200. train_loss: 0.6830.. train_acc: 0.6200.. test_loss:0.6922.. test_acc:0.5301..\n",
      "Epoch 17/200. train_loss: 0.6973.. train_acc: 0.4800.. test_loss:0.6909.. test_acc:0.5303..\n",
      "Epoch 17/200. train_loss: 0.6754.. train_acc: 0.6700.. test_loss:0.6925.. test_acc:0.5328..\n",
      "Epoch 17/200. train_loss: 0.6912.. train_acc: 0.5300.. test_loss:0.6911.. test_acc:0.5300..\n",
      "Epoch 18/200. train_loss: 0.7007.. train_acc: 0.4900.. test_loss:0.6913.. test_acc:0.5309..\n",
      "Epoch 18/200. train_loss: 0.6931.. train_acc: 0.5100.. test_loss:0.6897.. test_acc:0.5372..\n",
      "Epoch 18/200. train_loss: 0.6863.. train_acc: 0.5500.. test_loss:0.6906.. test_acc:0.5362..\n",
      "Epoch 19/200. train_loss: 0.7029.. train_acc: 0.4700.. test_loss:0.6901.. test_acc:0.5354..\n",
      "Epoch 19/200. train_loss: 0.6977.. train_acc: 0.4900.. test_loss:0.6902.. test_acc:0.5354..\n",
      "Epoch 19/200. train_loss: 0.6864.. train_acc: 0.5800.. test_loss:0.6901.. test_acc:0.5393..\n",
      "Epoch 20/200. train_loss: 0.6934.. train_acc: 0.5200.. test_loss:0.6895.. test_acc:0.5399..\n",
      "Epoch 20/200. train_loss: 0.6911.. train_acc: 0.5400.. test_loss:0.6899.. test_acc:0.5369..\n",
      "Epoch 21/200. train_loss: 0.6944.. train_acc: 0.5500.. test_loss:0.6896.. test_acc:0.5376..\n",
      "Epoch 21/200. train_loss: 0.7050.. train_acc: 0.5000.. test_loss:0.6889.. test_acc:0.5393..\n",
      "Epoch 21/200. train_loss: 0.6780.. train_acc: 0.6000.. test_loss:0.6888.. test_acc:0.5399..\n",
      "Epoch 22/200. train_loss: 0.6833.. train_acc: 0.5800.. test_loss:0.6884.. test_acc:0.5399..\n",
      "Epoch 22/200. train_loss: 0.6658.. train_acc: 0.6200.. test_loss:0.6880.. test_acc:0.5395..\n",
      "Epoch 22/200. train_loss: 0.6890.. train_acc: 0.5600.. test_loss:0.6886.. test_acc:0.5367..\n",
      "Epoch 23/200. train_loss: 0.7079.. train_acc: 0.4800.. test_loss:0.6883.. test_acc:0.5365..\n",
      "Epoch 23/200. train_loss: 0.6991.. train_acc: 0.4700.. test_loss:0.6876.. test_acc:0.5428..\n",
      "Epoch 23/200. train_loss: 0.7172.. train_acc: 0.4100.. test_loss:0.6866.. test_acc:0.5459..\n",
      "Epoch 24/200. train_loss: 0.6807.. train_acc: 0.5600.. test_loss:0.6880.. test_acc:0.5438..\n",
      "Epoch 24/200. train_loss: 0.6891.. train_acc: 0.5500.. test_loss:0.6870.. test_acc:0.5416..\n",
      "Epoch 24/200. train_loss: 0.6767.. train_acc: 0.5600.. test_loss:0.6877.. test_acc:0.5464..\n",
      "Epoch 25/200. train_loss: 0.6928.. train_acc: 0.6100.. test_loss:0.6866.. test_acc:0.5459..\n",
      "Epoch 25/200. train_loss: 0.6916.. train_acc: 0.5800.. test_loss:0.6864.. test_acc:0.5461..\n",
      "Epoch 25/200. train_loss: 0.6739.. train_acc: 0.5500.. test_loss:0.6860.. test_acc:0.5505..\n",
      "Epoch 26/200. train_loss: 0.7001.. train_acc: 0.5800.. test_loss:0.6859.. test_acc:0.5450..\n",
      "Epoch 26/200. train_loss: 0.6821.. train_acc: 0.5600.. test_loss:0.6847.. test_acc:0.5498..\n",
      "Epoch 26/200. train_loss: 0.6945.. train_acc: 0.5500.. test_loss:0.6867.. test_acc:0.5444..\n",
      "Epoch 27/200. train_loss: 0.6852.. train_acc: 0.5200.. test_loss:0.6860.. test_acc:0.5489..\n",
      "Epoch 27/200. train_loss: 0.7044.. train_acc: 0.4700.. test_loss:0.6846.. test_acc:0.5527..\n",
      "Epoch 27/200. train_loss: 0.6804.. train_acc: 0.5700.. test_loss:0.6851.. test_acc:0.5508..\n",
      "Epoch 28/200. train_loss: 0.6831.. train_acc: 0.5800.. test_loss:0.6850.. test_acc:0.5490..\n",
      "Epoch 28/200. train_loss: 0.7015.. train_acc: 0.4900.. test_loss:0.6851.. test_acc:0.5510..\n",
      "Epoch 28/200. train_loss: 0.6811.. train_acc: 0.6000.. test_loss:0.6839.. test_acc:0.5541..\n",
      "Epoch 29/200. train_loss: 0.7018.. train_acc: 0.5300.. test_loss:0.6837.. test_acc:0.5504..\n",
      "Epoch 29/200. train_loss: 0.7038.. train_acc: 0.5200.. test_loss:0.6851.. test_acc:0.5518..\n",
      "Epoch 30/200. train_loss: 0.6765.. train_acc: 0.6000.. test_loss:0.6842.. test_acc:0.5480..\n",
      "Epoch 30/200. train_loss: 0.6855.. train_acc: 0.5900.. test_loss:0.6828.. test_acc:0.5549..\n",
      "Epoch 30/200. train_loss: 0.6560.. train_acc: 0.6100.. test_loss:0.6847.. test_acc:0.5562..\n",
      "Epoch 31/200. train_loss: 0.6789.. train_acc: 0.4600.. test_loss:0.6836.. test_acc:0.5534..\n",
      "Epoch 31/200. train_loss: 0.7020.. train_acc: 0.4900.. test_loss:0.6826.. test_acc:0.5550..\n",
      "Epoch 31/200. train_loss: 0.6987.. train_acc: 0.4700.. test_loss:0.6824.. test_acc:0.5541..\n",
      "Epoch 32/200. train_loss: 0.6869.. train_acc: 0.5300.. test_loss:0.6816.. test_acc:0.5550..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200. train_loss: 0.6543.. train_acc: 0.6100.. test_loss:0.6829.. test_acc:0.5565..\n",
      "Epoch 32/200. train_loss: 0.6641.. train_acc: 0.6600.. test_loss:0.6829.. test_acc:0.5549..\n",
      "Epoch 33/200. train_loss: 0.6947.. train_acc: 0.5000.. test_loss:0.6820.. test_acc:0.5566..\n",
      "Epoch 33/200. train_loss: 0.6704.. train_acc: 0.5600.. test_loss:0.6827.. test_acc:0.5569..\n",
      "Epoch 33/200. train_loss: 0.7174.. train_acc: 0.5300.. test_loss:0.6826.. test_acc:0.5568..\n",
      "Epoch 34/200. train_loss: 0.6720.. train_acc: 0.5900.. test_loss:0.6822.. test_acc:0.5551..\n",
      "Epoch 34/200. train_loss: 0.6658.. train_acc: 0.5800.. test_loss:0.6809.. test_acc:0.5573..\n",
      "Epoch 34/200. train_loss: 0.6566.. train_acc: 0.6400.. test_loss:0.6815.. test_acc:0.5589..\n",
      "Epoch 35/200. train_loss: 0.6680.. train_acc: 0.5500.. test_loss:0.6813.. test_acc:0.5566..\n",
      "Epoch 35/200. train_loss: 0.6652.. train_acc: 0.6000.. test_loss:0.6813.. test_acc:0.5561..\n",
      "Epoch 35/200. train_loss: 0.6892.. train_acc: 0.5000.. test_loss:0.6819.. test_acc:0.5571..\n",
      "Epoch 36/200. train_loss: 0.6725.. train_acc: 0.5900.. test_loss:0.6801.. test_acc:0.5611..\n",
      "Epoch 36/200. train_loss: 0.6645.. train_acc: 0.6100.. test_loss:0.6814.. test_acc:0.5561..\n",
      "Epoch 36/200. train_loss: 0.6617.. train_acc: 0.5600.. test_loss:0.6803.. test_acc:0.5591..\n",
      "Epoch 37/200. train_loss: 0.6789.. train_acc: 0.5100.. test_loss:0.6795.. test_acc:0.5632..\n",
      "Epoch 37/200. train_loss: 0.6593.. train_acc: 0.6200.. test_loss:0.6801.. test_acc:0.5595..\n",
      "Epoch 37/200. train_loss: 0.6597.. train_acc: 0.6500.. test_loss:0.6803.. test_acc:0.5596..\n",
      "Epoch 38/200. train_loss: 0.6861.. train_acc: 0.5700.. test_loss:0.6810.. test_acc:0.5562..\n",
      "Epoch 38/200. train_loss: 0.6501.. train_acc: 0.6600.. test_loss:0.6792.. test_acc:0.5632..\n",
      "Epoch 38/200. train_loss: 0.6802.. train_acc: 0.5400.. test_loss:0.6796.. test_acc:0.5624..\n",
      "Epoch 39/200. train_loss: 0.6767.. train_acc: 0.5600.. test_loss:0.6813.. test_acc:0.5601..\n",
      "Epoch 39/200. train_loss: 0.6383.. train_acc: 0.6800.. test_loss:0.6796.. test_acc:0.5618..\n",
      "Epoch 40/200. train_loss: 0.6816.. train_acc: 0.5400.. test_loss:0.6808.. test_acc:0.5624..\n",
      "Epoch 40/200. train_loss: 0.6569.. train_acc: 0.6400.. test_loss:0.6814.. test_acc:0.5605..\n",
      "Epoch 40/200. train_loss: 0.6879.. train_acc: 0.5100.. test_loss:0.6791.. test_acc:0.5630..\n",
      "Epoch 41/200. train_loss: 0.6997.. train_acc: 0.5200.. test_loss:0.6779.. test_acc:0.5672..\n",
      "Epoch 41/200. train_loss: 0.6831.. train_acc: 0.5400.. test_loss:0.6790.. test_acc:0.5642..\n",
      "Epoch 41/200. train_loss: 0.6561.. train_acc: 0.5800.. test_loss:0.6802.. test_acc:0.5643..\n",
      "Epoch 42/200. train_loss: 0.6464.. train_acc: 0.5900.. test_loss:0.6785.. test_acc:0.5642..\n",
      "Epoch 42/200. train_loss: 0.6705.. train_acc: 0.6000.. test_loss:0.6781.. test_acc:0.5662..\n",
      "Epoch 42/200. train_loss: 0.6940.. train_acc: 0.4900.. test_loss:0.6777.. test_acc:0.5643..\n",
      "Epoch 43/200. train_loss: 0.6640.. train_acc: 0.6500.. test_loss:0.6779.. test_acc:0.5657..\n",
      "Epoch 43/200. train_loss: 0.6818.. train_acc: 0.6200.. test_loss:0.6779.. test_acc:0.5663..\n",
      "Epoch 43/200. train_loss: 0.6638.. train_acc: 0.5500.. test_loss:0.6769.. test_acc:0.5665..\n",
      "Epoch 44/200. train_loss: 0.7079.. train_acc: 0.5500.. test_loss:0.6771.. test_acc:0.5682..\n",
      "Epoch 44/200. train_loss: 0.7096.. train_acc: 0.5400.. test_loss:0.6778.. test_acc:0.5631..\n",
      "Epoch 44/200. train_loss: 0.7228.. train_acc: 0.4300.. test_loss:0.6783.. test_acc:0.5657..\n",
      "Epoch 45/200. train_loss: 0.6764.. train_acc: 0.5500.. test_loss:0.6765.. test_acc:0.5716..\n",
      "Epoch 45/200. train_loss: 0.6948.. train_acc: 0.5200.. test_loss:0.6777.. test_acc:0.5627..\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "keep_prob = 0.9\n",
    "epochs = 200\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    train(X_train,Y_train,X_test,Y_test,keep_prob,epochs,batch_size,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
